{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91dfab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pycocotools\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e67e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():  \n",
    "#     device = \"cuda:0\" \n",
    "# else:  \n",
    "#     device = \"cpu\" \n",
    "    \n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e592f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Maastricht to access GPU of choice\n",
    "\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06cbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device(f'cuda:{0}' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258a21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device(), torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bce94",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3c8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "# Class for a customized dataset\n",
    "# In this case preprocessed CEM images combined in a 3-channel RGB .jpg format\n",
    "# and the corresponding mask of present lesions in a 1-channel .png format\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root, annotations_file, img_dir, mask_dir, train=False, transform=None, target_transform=None):\n",
    "        # Read the .csv file with all the information\n",
    "        self.img_labels = pd.read_csv(os.path.join(root, annotations_file))\n",
    "        # Define the directories of the images and masks\n",
    "        self.img_dir = os.path.join(root, img_dir)\n",
    "        self.mask_dir = os.path.join(root, mask_dir)\n",
    "        # Define whethe transformations are included\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of cases in the dataset\n",
    "        # In this set, CC and MLO of the same breast are considered different cases\n",
    "        \n",
    "        self.setlen = len(self.img_labels)\n",
    "        self.auglen = len(self.img_labels)*4\n",
    "        return len(self.img_labels)*4\n",
    "\n",
    "    def __getitem__(self, augidx):\n",
    "        idx = augidx % self.setlen\n",
    "        print('patient', idx, augidx)\n",
    "        \n",
    "        # Read the image and the mask for a case from the directories\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        mask_path = self.img_labels.iloc[idx,6]\n",
    "        image = read_image(img_path).float()\n",
    "        mask = read_image(mask_path)\n",
    "        \n",
    "        xmin = self.img_labels.iloc[idx, 1]\n",
    "        xmax = self.img_labels.iloc[idx, 2]\n",
    "        ymin = self.img_labels.iloc[idx, 3]\n",
    "        ymax = self.img_labels.iloc[idx, 4]\n",
    "        box_in_patch = True\n",
    "#         print('Init', image.shape, mask.shape, np.unique(mask), xmin,ymin,xmax,ymax)\n",
    "        \n",
    "        # use quadrant for data augmentation and to limit resizing\n",
    "        if augidx <= self.setlen :\n",
    "            # upper left\n",
    "#             print('upper left', int(image.shape[1]/2), int(image.shape[2]/2))\n",
    "            \n",
    "            if (xmin > int(image.shape[1]/2)) or (ymin > int(image.shape[2]/2)) :\n",
    "                box_in_patch = False\n",
    "            else : \n",
    "                xmax = min(xmax, int(image.shape[1]/2))\n",
    "                ymax = min(ymax, int(image.shape[2]/2))   \n",
    "            \n",
    "            mask = mask[:, :int(image.shape[1]/2), :int(image.shape[2]/2)]\n",
    "            image = image[:, :int(image.shape[1]/2), :int(image.shape[2]/2)]\n",
    "                \n",
    "        elif self.setlen < augidx <= self.setlen*2 :\n",
    "            # bottom left\n",
    "#             print('bottom left', int(image.shape[1]/2), int(image.shape[2]/2))\n",
    "            \n",
    "            if (xmax < int(image.shape[1]/2)) or (ymin > int(image.shape[2]/2)) :\n",
    "                box_in_patch = False\n",
    "            else : \n",
    "                xmin = max(xmin, int(image.shape[1]/2))\n",
    "                ymax = min(ymax, int(image.shape[2]/2))\n",
    "                \n",
    "                xmin -= int(image.shape[1]/2)\n",
    "                xmax -= int(image.shape[1]/2)\n",
    "            \n",
    "            mask = mask[:, int(image.shape[1]/2):, :int(image.shape[2]/2)]\n",
    "            image = image[:, int(image.shape[1]/2):, :int(image.shape[2]/2)]\n",
    "                \n",
    "        elif self.setlen*2 < augidx <= self.setlen*3 :\n",
    "            # upper righgt\n",
    "#             print('upper right', int(image.shape[1]/2), int(image.shape[2]/2))\n",
    "            \n",
    "            if (xmin > int(image.shape[1]/2)) or (ymax < int(image.shape[2]/2)) :\n",
    "                box_in_patch = False\n",
    "            else : \n",
    "                xmax = min(xmax, int(image.shape[1]/2))\n",
    "                ymin = max(ymin, int(image.shape[2]/2))\n",
    "                \n",
    "                ymin -= int(image.shape[2]/2)\n",
    "                ymax -= int(image.shape[2]/2)\n",
    "            \n",
    "            mask = mask[:, :int(image.shape[1]/2), int(image.shape[2]/2):]\n",
    "            image = image[:, :int(image.shape[1]/2), int(image.shape[2]/2):]\n",
    "                \n",
    "        elif self.setlen*3 < augidx <= self.setlen*4 :\n",
    "            # bottom right\n",
    "#             print('bottom right', int(image.shape[1]/2), int(image.shape[2]/2))\n",
    "            \n",
    "            if (xmax < int(image.shape[1]/2)) or (ymax < int(image.shape[2]/2)) :\n",
    "                box_in_patch = False\n",
    "            else : \n",
    "                xmin = max(xmin, int(image.shape[1]/2))\n",
    "                ymin = max(ymin, int(image.shape[2]/2))\n",
    "                \n",
    "                xmin -= int(image.shape[1]/2)\n",
    "                xmax -= int(image.shape[1]/2)\n",
    "                ymin -= int(image.shape[2]/2)\n",
    "                ymax -= int(image.shape[2]/2)\n",
    "            \n",
    "            mask = mask[:, int(image.shape[1]/2):, int(image.shape[2]/2):]\n",
    "            image = image[:, int(image.shape[1]/2):, int(image.shape[2]/2):]            \n",
    "                \n",
    "        print(box_in_patch)\n",
    "#         print('Patch', image.shape, mask.shape, np.unique(mask), xmin,ymin,xmax,ymax)\n",
    "        \n",
    "        # Apply transformations if defined\n",
    "        flipint = random.random()\n",
    "        if self.train and flipint > 0.5 :        \n",
    "            image = T.RandomHorizontalFlip(p=1.0)(image)\n",
    "            mask = T.RandomHorizontalFlip(p=1.0)(mask)\n",
    "            \n",
    "            ymax_temp = image.shape[2] - ymin\n",
    "            ymin = image.shape[2] - ymax\n",
    "            ymax = ymax_temp\n",
    "#             xmax_temp = image.shape[1] - xmin\n",
    "#             xmin = image.shape[1] - xmax\n",
    "#             xmax = xmax_temp\n",
    "\n",
    "#         print('Flip', image.shape, mask.shape, np.unique(mask), xmin,ymin,xmax,ymax)\n",
    "        # Resize so all images and masks have the same size\n",
    "#         image = T.Resize([800,800])(image)\n",
    "#         mask = T.Resize([800,800])(mask)    \n",
    "#         resize_scale_x = 800/image.size()[1]\n",
    "#         resize_scale_y = 800/image.size()[2]\n",
    "\n",
    "        # Resize if necessary\n",
    "        # First the smallest dimension is reduced to 400 if it is larger\n",
    "        # Then the largest dimension is reduced to 650 if it is still larger\n",
    "        resize_scale = 1.0\n",
    "        min_size_idx = np.argmin([image.size()[1], image.size()[2]])\n",
    "        \n",
    "        if min_size_idx == 0 and image.size()[1] > 400 :\n",
    "            resize_scale *= 400/image.size()[1]\n",
    "            image = T.Resize([400, int(400*image.size()[2]/image.size()[1])])(image)\n",
    "            mask = T.Resize([400, int(400*image.size()[2]/image.size()[1])], interpolation=InterpolationMode.NEAREST)(mask)\n",
    "        if min_size_idx == 0 and image.size()[2] > 650 :\n",
    "            resize_scale *= 650/image.size()[2]\n",
    "            image = T.Resize([int(650*image.size()[1]/image.size()[2]), 650])(image)\n",
    "            mask = T.Resize([int(650*image.size()[1]/image.size()[2]), 650], interpolation=InterpolationMode.NEAREST)(mask)\n",
    "        \n",
    "        if min_size_idx == 1 and image.size()[2] > 400 :\n",
    "            resize_scale *= 400/image.size()[2]\n",
    "            image = T.Resize([int(400*image.size()[1]/image.size()[2]), 400])(image)\n",
    "            mask = T.Resize([int(400*image.size()[1]/image.size()[2]), 400], interpolation=InterpolationMode.NEAREST)(mask)\n",
    "        if min_size_idx == 1 and image.size()[1] > 650 :\n",
    "            resize_scale *= 650/image.size()[1]\n",
    "            image = T.Resize([650, int(650*image.size()[2]/image.size()[1])])(image)\n",
    "            mask = T.Resize([650, int(650*image.size()[2]/image.size()[1])], interpolation=InterpolationMode.NEAREST)(mask)\n",
    "            \n",
    "#         print('Resize', image.shape, mask.shape, np.unique(mask), xmin,ymin,xmax,ymax)\n",
    "        # Normalize image with mean and standard deviation per channel\n",
    "        mean = torch.mean(image, dim=(1,2))\n",
    "        stdev = torch.std(image, dim=(1,2))\n",
    "        image = T.Normalize(mean, stdev)(image)\n",
    "        \n",
    "        # Rescale to [0,1] range per channel\n",
    "        for dim in range(3) :\n",
    "            image[dim] -= torch.min(image[dim])\n",
    "            image[dim] /= torch.max(image[dim])  \n",
    "        \n",
    "        # Read the location of the lesion bounding box from the .csv file\n",
    "        if box_in_patch :            \n",
    "            # Create separate channel in mask for each lesion\n",
    "#             mask_out = np.zeros((torch.max(mask).item(), mask.shape[-2], mask.shape[-1]))\n",
    "            mask_out = np.zeros((1, mask.shape[-2], mask.shape[-1]))\n",
    "#             for lesion_idx in range(torch.max(mask).item()) :\n",
    "            for lesion_idx in range(1) :\n",
    "    #             mask_out[lesion_idx][mask[0]==lesion_idx+1] = 1  \n",
    "                mask_out[lesion_idx][mask[0]>0] = 1 # alternative for wrong masks with only one lesion\n",
    "            \n",
    "            boxes = [[ymin*resize_scale, xmin*resize_scale, ymax*resize_scale, xmax*resize_scale]]\n",
    "            area = (boxes[0][3] - boxes[0][1]) * (boxes[0][2] - boxes[0][0])\n",
    "        else :\n",
    "            mask_out = np.zeros_like(mask)\n",
    "            \n",
    "            boxes = [[]]\n",
    "            area = 0\n",
    "        # Read the label of the lesion from the .csv file\n",
    "        labels = self.img_labels.iloc[idx, 5]  + 1   # 0 represents background class, thus 1=benign, 2=malignant\n",
    "#         print('Labels', image.shape, mask_out.shape, np.unique(mask_out), xmin*resize_scale,ymin*resize_scale,xmax*resize_scale,ymax*resize_scale)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        labels = torch.tensor([labels])\n",
    "        image_id = torch.tensor([idx])\n",
    "       \n",
    "        mask_out = torch.from_numpy(mask_out)\n",
    "        mask_out = mask_out.to(torch.uint8)\n",
    "#         print('Out', image.shape, mask_out.shape, np.unique(mask_out), xmin*resize_scale,ymin*resize_scale,xmax*resize_scale,ymax*resize_scale)\n",
    "        \n",
    "        iscrowd = torch.zeros((2,), dtype=torch.int64)  \n",
    "            \n",
    "        target = {}\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"masks\"] = mask_out\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"area\"] = area\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "            \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25b19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from monai.transforms import CropForeground, RandSpatialCropSamplesd, Flipd, RandCropByPosNegLabeld, Resized, FillHoles, NormalizeIntensityd\n",
    "from monai.utils.enums import InterpolateMode\n",
    "\n",
    "# Define the transformations of Monai to be used later on\n",
    "resize_fcn_large = Resized([\"image\",\"mask\"],\n",
    "                     spatial_size=(1300,800),\n",
    "                     mode=[InterpolateMode.BILINEAR,InterpolateMode.NEAREST])\n",
    "resize_fcn_small = Resized([\"image\",\"mask\"],\n",
    "                     spatial_size=(650,400),\n",
    "                     mode=[InterpolateMode.BILINEAR,InterpolateMode.NEAREST])\n",
    "# crops_fcn = RandSpatialCropSamplesd([\"image\",\"mask\"],\n",
    "#                                     num_samples=10,\n",
    "#                                     roi_size=(650,400),\n",
    "#                                     random_size=False)\n",
    "crop_fcn_small = RandCropByPosNegLabeld([\"image\", \"mask\"],\n",
    "                                  \"mask\",\n",
    "                                  (650,400),\n",
    "                                  pos=0.1, neg=1.0,\n",
    "                                  num_samples=10)\n",
    "flip_fcn = Flipd([\"image\",\"mask\"],\n",
    "                 spatial_axis=1)\n",
    "fill_fcn = FillHoles()\n",
    "norm_fcn = NormalizeIntensityd([\"image\"], channel_wise=True, nonzero=False)\n",
    "\n",
    "# Class for a customized dataset\n",
    "# In this case preprocessed CEM images combined in a 3-channel RGB .jpg format\n",
    "# and the corresponding mask of present lesions in a 1-channel .png format\n",
    "class CustomImageDatasetMonai(Dataset):\n",
    "    def __init__(self, root, annotations_file, img_dir, mask_dir, train=False, transform=None, target_transform=None):\n",
    "        # Read the .csv file with all the information\n",
    "        self.img_labels = pd.read_csv(os.path.join(root, annotations_file))\n",
    "        # Define the directories of the images and masks\n",
    "        self.img_dir = os.path.join(root, img_dir)\n",
    "        self.mask_dir = os.path.join(root, mask_dir)\n",
    "        # Define whether transformations are included\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of cases in the dataset\n",
    "        # In this set, CC and MLO of the same breast are considered different cases\n",
    "        \n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Read the image and the mask for a case from the directories\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        mask_path = self.img_labels.iloc[idx,6]\n",
    "        image = read_image(img_path).float()\n",
    "        mask = read_image(mask_path)\n",
    "        \n",
    "        print('patient', idx, img_path)\n",
    "        \n",
    "        init_dict = {}\n",
    "        init_dict[\"image\"] = image\n",
    "        init_dict[\"mask\"] = mask\n",
    "        print('Init', init_dict[\"mask\"].shape, np.unique(init_dict[\"mask\"],return_counts=True))\n",
    "        \n",
    "#         # Resize the image to 1300 by 800\n",
    "#         init_dict = resize_fcn(init_dict)\n",
    "        \n",
    "        # Create 10 samples of croppped image and mask\n",
    "#         crop_dict = crops_fcn(init_dict)\n",
    "#         print(crop_dict[0][\"image\"].shape)\n",
    "        # Find the first sample with a nonzero mask\n",
    "    \n",
    "        crop_fcn_large = RandCropByPosNegLabeld([\"image\", \"mask\"],\n",
    "                                                \"mask\",\n",
    "                                                (int(mask.shape[1]/2),int(mask.shape[2]/2)),\n",
    "                                                pos=0.1, neg=1.0,\n",
    "                                                num_samples=10)\n",
    "    \n",
    "        # Generate 10 croppped samples until a nonzero mask is obtained\n",
    "        # Then choose the sample with the largest nonzero region\n",
    "        max_pos = 0\n",
    "        max_pos_idx = 0\n",
    "        while max_pos == 0 :\n",
    "            crop_dict = crop_fcn_large(init_dict)\n",
    "            for crop_idx in range(10) :\n",
    "#                 print(crop_idx, np.count_nonzero(crop_dict[crop_idx][\"mask\"]), crop_dict[crop_idx][\"mask\"].shape)\n",
    "                if np.count_nonzero(crop_dict[crop_idx][\"mask\"]) > max(max_pos, mask.nelement()/40000) :\n",
    "                    max_pos = np.count_nonzero(crop_dict[crop_idx][\"mask\"])\n",
    "                    max_pos_idx = crop_idx\n",
    "            \n",
    "        print('Crop', crop_dict[max_pos_idx][\"mask\"].shape, np.unique(crop_dict[max_pos_idx][\"mask\"],return_counts=True))\n",
    "        \n",
    "        resize_dict = resize_fcn_small(crop_dict[max_pos_idx])\n",
    "        print('Resize', resize_dict[\"mask\"].shape, np.unique(resize_dict[\"mask\"],return_counts=True))\n",
    "        \n",
    "        # Data augmentation by horizontal flipping\n",
    "        flipint = random.random()\n",
    "        if self.train and flipint > 0.5 : \n",
    "            resize_dict = flip_fcn(resize_dict)\n",
    "        else :\n",
    "            resize_dict = resize_dict\n",
    "        print('Flip', resize_dict[\"mask\"].shape, np.unique(resize_dict[\"mask\"],return_counts=True))\n",
    "            \n",
    "        # Normalize image as values should be in [0,1]\n",
    "        min_vals = [torch.min(resize_dict[\"image\"][0]).item(), torch.min(resize_dict[\"image\"][1]).item(), torch.min(resize_dict[\"image\"][2]).item()]\n",
    "        max_vals = [torch.max(resize_dict[\"image\"][0]).item(), torch.max(resize_dict[\"image\"][1]).item(), torch.max(resize_dict[\"image\"][2]).item()]\n",
    "        norm_minmax_fcn = NormalizeIntensityd([\"image\"],\n",
    "                                              min_vals, [a_i-b_i for a_i,b_i in zip(max_vals,min_vals)],\n",
    "                                              channel_wise=True, nonzero=False)\n",
    "        \n",
    "#         resize_dict = norm_fcn(resize_dict)\n",
    "        resize_dict = norm_minmax_fcn(resize_dict)\n",
    "#         print('Resize', resize_dict[\"mask\"].shape, np.unique(resize_dict[\"mask\"],return_counts=True), np.unique(resize_dict[\"image\"]))\n",
    "                        \n",
    "        mask_out = np.zeros((1, resize_dict[\"mask\"].shape[-2], resize_dict[\"mask\"].shape[-1]))\n",
    "        mask_out[0][resize_dict[\"mask\"][0]>0] = 1 # alternative for wrong masks with only one lesion\n",
    "        print('Mask out', mask_out.shape, np.unique(mask_out,return_counts=True))\n",
    "            \n",
    "        # Find the new bounding box on the transformed image\n",
    "        bbox = CropForeground().compute_bounding_box(resize_dict[\"mask\"])\n",
    "        print(bbox)\n",
    "        bbox_vals = [[bbox[0][1], bbox[0][0], bbox[1][1], bbox[1][0]]]\n",
    "        area_vals = [(bbox_vals[0][3] - bbox_vals[0][1]) * (bbox_vals[0][2] - bbox_vals[0][0])]      \n",
    "        labels = self.img_labels.iloc[idx, 5]  + 1   # 0 represents background class, thus 1=benign, 2=malignant\n",
    "        iscrowd = torch.zeros((2,), dtype=torch.int64)  \n",
    "        print(area_vals)\n",
    "            \n",
    "        # Create the target dictionary\n",
    "        target = {}\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"masks\"] = torch.from_numpy(mask_out).to(torch.uint8)\n",
    "        target[\"boxes\"] = torch.as_tensor(bbox_vals, dtype=torch.float32)\n",
    "        target[\"area\"] = torch.as_tensor(area_vals, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.tensor([torch.as_tensor(labels, dtype=torch.int64)])\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return resize_dict[\"image\"], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd804ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train=False) :\n",
    "    if train :\n",
    "        transforms = torch.nn.Sequential(T.RandomHorizontalFlip(p=0.5))\n",
    "        \n",
    "        return transforms\n",
    "        \n",
    "    else : \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f89d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traindatadir = r'B:\\Astrid\\Preprocessed\\220615_preprocessed\\realAll'\n",
    "trainval_data = CustomImageDatasetMonai(traindatadir, 'annotations_train_real.csv', 'colored_to_jpg', 'mask_to_png', train=True)\n",
    "\n",
    "testdatadir = r'B:\\Astrid\\Preprocessed\\220615_preprocessed\\realAll'\n",
    "test_data = CustomImageDatasetMonai(testdatadir, 'annotations_test_real.csv', 'colored_to_jpg', 'mask_to_png')\n",
    "\n",
    "savedir = 'B:\\\\Astrid\\\\Preprocessed\\\\ModelsAll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ea91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import utils\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Read training dataset and split in training and validation data\n",
    "# Use same random split every time\n",
    "# train_data_nonflip, val_data = random_split(trainval_data_nonflip, [int(np.floor(len(trainval_data_nonflip)*0.8)), int(np.ceil(len(trainval_data_nonflip)*0.2))], generator=torch.Generator().manual_seed(0))\n",
    "# train_data_flip, _ = random_split(trainval_data_flip, [int(np.floor(len(trainval_data_flip)*0.8)), int(np.ceil(len(trainval_data_flip)*0.2))], generator=torch.Generator().manual_seed(0))\n",
    "# train_data = torch.utils.data.ConcatDataset([train_data_nonflip, train_data_flip])\n",
    "\n",
    "train_data, val_data = random_split(trainval_data, [int(np.floor(len(trainval_data)*0.8)), int(np.ceil(len(trainval_data)*0.2))], generator=torch.Generator().manual_seed(0))\n",
    "# train_data = trainval_data\n",
    "# val_data = test_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Read test dataset\n",
    "test_dataloader = DataLoader(test_data, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4c9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4739d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd1939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training\n",
    "train_images, train_targets = next(train_iter)\n",
    "train_image_list = list(image for image in train_images)\n",
    "train_target_list = [{k: v.to('cpu') for k, v in t.items()} for t in train_targets]\n",
    "\n",
    "# For Validation\n",
    "val_images, val_targets = next(iter(val_dataloader))\n",
    "val_image_list = list(image for image in val_images)\n",
    "val_target_list = [{k: v for k, v in t.items()} for t in val_targets]\n",
    "\n",
    "# For Testing\n",
    "test_images, test_targets = next(iter(test_dataloader))\n",
    "test_image_list = list(image for image in test_images)\n",
    "test_target_list = [{k: v for k, v in t.items()} for t in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cefe290",
   "metadata": {},
   "outputs": [],
   "source": [
    "(top_left, top_right, bottom_left, bottom_right, center) = T.FiveCrop(size=(int(train_images[0].shape[1]/2), int(train_images[0].shape[2]/2)))(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "052a55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3)\n",
    "\n",
    "ax[0][0].imshow(top_left[0])\n",
    "ax[0][2].imshow(top_right[0])\n",
    "ax[1][1].imshow(center[0])\n",
    "ax[2][0].imshow(bottom_left[0])\n",
    "ax[2][2].imshow(bottom_right[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0285824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.empty(3, 4, 5)\n",
    "t.nelement()\n",
    "# torch.Size([3, 4, 5])\n",
    "# t.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a1e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "foobar.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b477454",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00951437",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e6a4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets[0]['image_id'], train_targets[1]['image_id'], train_targets[2]['image_id'], train_targets[3]['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d56816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(4) :\n",
    "    print(train_targets[idx]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7017ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "98e6133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "081eeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(train_images[0][0]).item(), torch.min(train_images[0][1]), torch.min(train_images[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d03dda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_idx = 0\n",
    "print(train_targets[pat_idx]['image_id'], train_targets[pat_idx]['boxes'])\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,15))\n",
    "\n",
    "ax[0].imshow(train_images[pat_idx][0])\n",
    "ax[1].imshow(train_targets[pat_idx]['masks'][0])\n",
    "\n",
    "enlarged_box = [int(train_targets[pat_idx]['boxes'][0][1])-50,\n",
    "                int(train_targets[pat_idx]['boxes'][0][3])+50,\n",
    "                int(train_targets[pat_idx]['boxes'][0][0])-50,                \n",
    "                int(train_targets[pat_idx]['boxes'][0][2])+50]\n",
    "print(enlarged_box)\n",
    "ax[2].imshow(train_images[pat_idx][0, enlarged_box[0]:enlarged_box[1], enlarged_box[2]:enlarged_box[3]])\n",
    "ax[3].imshow(train_targets[pat_idx]['masks'][0, enlarged_box[0]:enlarged_box[1], enlarged_box[2]:enlarged_box[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cffe4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_idx = 2\n",
    "print(val_targets[pat_idx]['image_id'], val_targets[pat_idx]['boxes'])\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,15))\n",
    "\n",
    "ax[0].imshow(val_images[pat_idx][0])\n",
    "ax[1].imshow(val_targets[pat_idx]['masks'][0])\n",
    "\n",
    "enlarged_box = [int(val_targets[pat_idx]['boxes'][0][1])-50,\n",
    "                int(val_targets[pat_idx]['boxes'][0][3])+50,\n",
    "                int(val_targets[pat_idx]['boxes'][0][0])-50,                \n",
    "                int(val_targets[pat_idx]['boxes'][0][2])+50]\n",
    "print(enlarged_box)\n",
    "ax[2].imshow(val_images[pat_idx][0, enlarged_box[0]:enlarged_box[1], enlarged_box[2]:enlarged_box[3]])\n",
    "ax[3].imshow(val_targets[pat_idx]['masks'][0, enlarged_box[0]:enlarged_box[1], enlarged_box[2]:enlarged_box[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a0fea",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7300fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                     output_size=14,\n",
    "                                                     sampling_ratio=2)\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# load an instance segmentation model pre-trained on COCO\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                           box_roi_pool=roi_pooler,\n",
    "                                                           mask_roi_pool=mask_roi_pooler,\n",
    "                                                           min_size=400,\n",
    "                                                           max_size=650,\n",
    "                                                           box_fg_iou_thresh=0.5,\n",
    "                                                           box_bg_iou_thresh=0.5)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "# now get the number of input features for the mask classifier\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "# and replace the mask predictor with a new one\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                   hidden_layer,\n",
    "                                                   num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f65067",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31063fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_kulum(model, optimizer, train_data_loader, val_data_loader, writer, device, epoch, print_freq, scaler=None):\n",
    "    \n",
    "    # Initialise training\n",
    "    model.train()\n",
    "    \n",
    "    # Set up logger to save metrics and losses\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    # Define scheduler for learning rate in optimizer\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(train_data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    # Read the images and targets from the training data loader\n",
    "    for images, targets in metric_logger.log_every(train_data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Compute the losses of the model on these training images and targets\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 50) # added since NaN in loss function\n",
    "#             torch.nn.utils.clip_grad_value_(model.parameters(), 50) # added since NaN in loss function\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "           \n",
    "        # Write to .tfevents\n",
    "#         grid = torchvision.utils.make_grid(images)\n",
    "#         writer.add_image('images', grid, 0)\n",
    "#         writer.add_graph(model, images)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', metric_logger.loss.value, epoch)\n",
    "        writer.add_scalar('Loss classifier/train', metric_logger.loss_classifier.value, epoch)\n",
    "        writer.add_scalar('Loss box reg/train', metric_logger.loss_box_reg.value, epoch)\n",
    "        writer.add_scalar('Loss mask/train', metric_logger.loss_mask.value, epoch)\n",
    "        writer.add_scalar('Loss objectness/train', metric_logger.loss_objectness.value, epoch)\n",
    "        writer.add_scalar('Loss rpn box reg/train', metric_logger.loss_rpn_box_reg.value, epoch)\n",
    "        \n",
    "    # Set up logger to save metrics and losses of validation\n",
    "    metric_logger_val = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger_val.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "        \n",
    "    # Read the images and targets from the validation data loader\n",
    "    for images_val, targets_val in metric_logger_val.log_every(val_data_loader, print_freq, header):\n",
    "        images_val = list(image.to(device) for image in images_val)\n",
    "        targets_val = [{k: v.to(device) for k, v in t.items()} for t in targets_val]\n",
    "        # Compute the losses of the model on these training images and targets\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict_val = model(images_val, targets_val)\n",
    "            losses_val = sum(loss for loss in loss_dict_val.values())\n",
    "\n",
    "        # Reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced_val = utils.reduce_dict(loss_dict_val)\n",
    "        losses_reduced_val = sum(loss for loss in loss_dict_reduced_val.values())\n",
    "\n",
    "        loss_value_val = losses_reduced_val.item()\n",
    "        \n",
    "        metric_logger_val.update(loss=losses_reduced_val, **loss_dict_reduced_val)\n",
    "        metric_logger_val.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        writer.add_scalar('Loss/val', metric_logger_val.loss.value, epoch)\n",
    "        writer.add_scalar('Loss classifier/val', metric_logger_val.loss_classifier.value, epoch)\n",
    "        writer.add_scalar('Loss box reg/val', metric_logger_val.loss_box_reg.value, epoch)\n",
    "        writer.add_scalar('Loss mask/val', metric_logger_val.loss_mask.value, epoch)\n",
    "        writer.add_scalar('Loss objectness/val', metric_logger_val.loss_objectness.value, epoch)\n",
    "        writer.add_scalar('Loss rpn box reg/val', metric_logger_val.loss_rpn_box_reg.value, epoch)\n",
    "\n",
    "    return metric_logger, metric_logger_val\n",
    "\n",
    "def early_stopping_kulum(prev_loss, curr_loss, num_epochs) :\n",
    "    \n",
    "    if curr_loss > prev_loss :\n",
    "        return num_epochs + 1\n",
    "    else :\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee231d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# # and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                step_size=3,\n",
    "#                                                gamma=0.1)\n",
    "\n",
    "# model_writer = SummaryWriter('\\\\\\\\tsclient\\\\E\\\\runs\\smallexpsynthetic')\n",
    "model_writer = SummaryWriter('B:\\\\Astrid\\\\Preprocessed\\\\runs\\\\102_Monai_resnet_benmal_all')\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 100\n",
    "logs = []\n",
    "logs_val = []\n",
    "min_val_loss = 0.0\n",
    "stop_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch', epoch)\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "#     sample_image, sample_target = sample(next(iter(train_dataloader)))\n",
    "    \n",
    "#     sample_image, sample_target = first(train_dataloader)\n",
    "#     output = self.model(sample_image.to(self.device))\n",
    "    \n",
    "    epoch_loss, epoch_loss_val = train_one_epoch_kulum(model, optimizer, train_dataloader, val_dataloader, model_writer, device, epoch, print_freq=10)\n",
    "#     epoch_loss = train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=10)\n",
    "    logs.append(epoch_loss)\n",
    "    logs_val.append(epoch_loss_val)\n",
    "    \n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "    # evaluate on the test dataset\n",
    "#     evaluate(model, val_dataloader, device=device)\n",
    "    \n",
    "#     torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': loss,\n",
    "#                 'loss_classifier': loss_classifier,\n",
    "#                 'loss_box_reg': loss_box_reg,\n",
    "#                 'loss_mask': loss_mask,\n",
    "#                 'loss_objectness': loss_objectness,\n",
    "#                 'loss_rpn_box': loss_rpn_box\n",
    "#                 }, savedir)\n",
    "\n",
    "#     if epoch > 50 :\n",
    "#         stop_epoch = early_stopping_kulum(min_val_loss, epoch_loss_val.loss.value, stop_epoch)\n",
    "#         if stop_epoch > 5 :\n",
    "#             print(f'Early stopping at epoch {epoch} with loss {epoch_loss_val.loss.value}.')\n",
    "\n",
    "#             break\n",
    "\n",
    "#         else :\n",
    "#             min_val_loss = min(min_val_loss, epoch_loss_val.loss.value)\n",
    "    \n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                    }, os.path.join(savedir, 'model102_Monai_clipval_resnet_benmal_all_'+str(epoch)+'.pth'))\n",
    "        \n",
    "model_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b63875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(savedir, 'dict100_Monai_resnet_benmal.dict'))\n",
    "torch.save(model, os.path.join(savedir, 'model100_Monai_resnet_benmal_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb1a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95fc4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test dataset\n",
    "testsynth_data = CustomImageDataset('B:\\\\Astrid\\\\Preprocessed\\\\TestCalcClusterSynthetic', 'annotations_test_calccluster_synthetic.csv', 'colored_to_jpg', 'mask_to_png')\n",
    "testsynth_dataloader = DataLoader(testsynth_data, batch_size=4, shuffle=True, collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa47bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, testsynth_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6bddf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_cuda = []\n",
    "for te in train_images :\n",
    "    train_images_cuda.append(te.to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aded286",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_cuda = []\n",
    "for te in val_images :\n",
    "    val_images_cuda.append(te.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ee57eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a161a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_cuda = []\n",
    "for te in test_images :\n",
    "    test_images_cuda.append(te.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7b81526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images.to(device)\n",
    "predictions = model(train_images_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d3fc03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "8e2f7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3f19d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[0].loss.value\n",
    "\n",
    "losses = []\n",
    "losses_class = []\n",
    "losses_box_reg = []\n",
    "losses_mask = []\n",
    "losses_objectness = []\n",
    "losses_rpn_box_reg = []\n",
    "for ep in range(len(logs)) :\n",
    "    losses.append(logs[ep].loss.value)\n",
    "    losses_class.append(logs[ep].loss_classifier.value)\n",
    "    losses_box_reg.append(logs[ep].loss_box_reg.value)\n",
    "    losses_mask.append(logs[ep].loss_mask.value)\n",
    "    losses_objectness.append(logs[ep].loss_objectness.value)\n",
    "    losses_rpn_box_reg.append(logs[ep].loss_rpn_box_reg.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5ff77a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "ax[0,0].plot(range(len(logs)),losses)\n",
    "ax[0,0].set_title('Loss')\n",
    "ax[0,1].plot(range(len(logs)),losses_class)\n",
    "ax[0,1].set_title('Loss_classifier')\n",
    "ax[0,2].plot(range(len(logs)),losses_box_reg)\n",
    "ax[0,2].set_title('Loss_box_reg')\n",
    "ax[1,0].plot(range(len(logs)),losses_mask)\n",
    "ax[1,0].set_title('Loss_mask')\n",
    "ax[1,1].plot(range(len(logs)),losses_objectness)\n",
    "ax[1,1].set_title('Loss_objectness')\n",
    "ax[1,2].plot(range(len(logs)),losses_rpn_box_reg)\n",
    "ax[1,2].set_title('Loss_rpn_box_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2147be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b364af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfecfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(predictions[2]['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d90095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': loss,\n",
    "#             ...\n",
    "#             }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07501cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0971a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (lesion) + background\n",
    "num_classes = 3  # 2 classes (lesion benign + lesion malignant) + background\n",
    "\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2b99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be0d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b3068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b22cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as vision_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8ba938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "output = model(image_list, target_list)   # Returns losses and detections\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3,300,400), torch.rand(3,500,400)]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67093915",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c053a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5015a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
